{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1\n",
    "\n",
    "# In a Nim learning model, the environment is the board where there are a certain number of items in three piles. The state is how much is in each stack/pile. In this case, the amount of stacks is 3, and the max items in the stacks is 10 items.  Basically, if they get the last item in the last pile, they lose. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2\n",
    "\n",
    "# The agent in the MIn model is the q learning agent, and essentailly this learning agent is basically trying to make the correct move to get the highest reward. Sometimes, this q learner can also explore the environment and decide between random or going with the high value. Essentially, the q learner will learn the actions, previous state, and the reward. The learner gets expereince from playing over and over and then will know what types of actions to do to get the reward and what not to do. The guru is also a quasi agent although it has just the perfect solution and does nothing else. It is essentially a programed agent with no other purpose.  Nim random is also an agent that makes random moves and carries out exploration \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#3\n",
    "\n",
    "# The reward for the model essentially grants a reward if they make a move that wins the game where they will get a reward of a positive integer typically, 1 but for this case defined in this case as 100. This is defined in qtable, Alpha, Gamma, Reward = None, 1.0, 0.8, 100.0. Typically the reward is some sort of postive reinforcement that would lead to more moves to obtain this award more often. Then the punishment is a negative integer. Essentially, rewards would usually cause the learner to make more positive moves to get the rewards\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#4 and 5\n",
    "\n",
    "# The number of possible states if there are max 10 items per pile and 3 piles total is (1+10) * (1+10) * (1+10) = 1331\n",
    "\n",
    "# The possible actions is 10*3 or 30 possible actions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from random import randint, choice\n",
    "\n",
    "# The number of piles is 3\n",
    "\n",
    "\n",
    "# max number of items per pile\n",
    "ITEMS_MX = 10\n",
    "\n",
    "# Initialize starting position\n",
    "def init_game():\n",
    "    return [randint(1,ITEMS_MX), randint(1,ITEMS_MX), randint(1,ITEMS_MX)]\n",
    "\n",
    "# Based on X-oring the item counts in piles - mathematical solution\n",
    "def nim_guru(st):\n",
    "    xored = st[0] ^ st[1] ^ st[2]\n",
    "    if xored == 0:\n",
    "        return nim_random(st)\n",
    "    for pile in range(3):\n",
    "        s = st[pile] ^ xored\n",
    "        if s <= st[pile]:\n",
    "            return st[pile]-s, pile\n",
    "\n",
    "# Random Nim player\n",
    "def nim_random(_st):\n",
    "    pile = choice([i for i in range(3) if _st[i]>0])  # find the non-empty piles\n",
    "    return randint(1, _st[pile]), pile  # random move\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def nim_qlearner(_st):\n",
    "    global qtable\n",
    "    # pick the best rewarding move, equation 1\n",
    "    a = np.argmax(qtable[_st[0], _st[1], _st[2]])  # exploitation\n",
    "    # index is based on move, pile\n",
    "    move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "    # check if qtable has generated a random but game illegal move - we have not explored there yet\n",
    "    if move <= 0 or _st[pile] < move:\n",
    "        move, pile = nim_random(_st)  # exploration\n",
    "    return move, pile  # action\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Engines = {'Random':nim_random, 'Guru':nim_guru, 'Qlearner':nim_qlearner}\n",
    "\n",
    "def game(a, b):\n",
    "    state, side = init_game(), 'A'\n",
    "    while True:\n",
    "        engine = Engines[a] if side == 'A' else Engines[b]\n",
    "        move, pile = engine(state)\n",
    "        # print(state, move, pile)  # debug purposes\n",
    "        state[pile] -= move\n",
    "        if state == [0, 0, 0]:  # game ends\n",
    "            return side  # winning side\n",
    "        side = 'B' if side == 'A' else 'A'  # switch sides\n",
    "\n",
    "def play_games(_n, a, b):\n",
    "    from collections import defaultdict\n",
    "    wins = defaultdict(int)\n",
    "    for i in range(_n):\n",
    "        wins[game(a, b)] += 1\n",
    "    # info\n",
    "    print(f\"{_n} games, {a:>8s}{wins['A']:5d}  {b:>8s}{wins['B']:5d}\")\n",
    "    return wins['A'], wins['B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print the entire set of states\n",
    "def qtable_log(_fn):\n",
    "    with open(_fn, 'w') as fout:\n",
    "        s = 'state'\n",
    "        for a in range(ITEMS_MX*3):\n",
    "            move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "            s += ',%02d_%01d' % (move,pile)\n",
    "        print(s, file=fout)\n",
    "        for i, j, k in [(i,j,k) for i in range(ITEMS_MX+1) for j in range(ITEMS_MX+1) for k in range(ITEMS_MX+1)]:\n",
    "            s = '%02d_%02d_%02d' % (i,j,k)\n",
    "            for a in range(ITEMS_MX*3):\n",
    "                r = qtable[i, j, k, a]\n",
    "                s += ',%.1f' % r\n",
    "            print(s, file=fout)\n",
    "\n",
    "qtable, Alpha, Gamma, Reward = None, 1.0, 0.8, 100.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# learn from _n games, randomly played to explore the possible states\n",
    "def nim_qlearn(_n):\n",
    "    global qtable\n",
    "    # based on max items per pile\n",
    "    qtable = np.zeros((ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX+1, ITEMS_MX*3), dtype=np.float32)\n",
    "    # play _n games\n",
    "    for i in range(_n):\n",
    "        # first state is starting position\n",
    "        st1 = init_game()\n",
    "        while True: # while game not finished\n",
    "        # make a random move - exploration\n",
    "            move, pile = nim_random(st1)\n",
    "            st2 = list(st1)\n",
    "            # make the move\n",
    "            st2[pile] -= move # --> last move I made\n",
    "            if st2 == [0, 0, 0]: # game ends\n",
    "                qtable_update(Reward, st1, move, pile, 0) # I won\n",
    "                break # new game\n",
    "    \n",
    "            elif np.max(qtable[st2[0], st2[1], st2[2]]) >= Reward:\n",
    "                # immediate loss - penalize it\n",
    "                qtable_update(-Reward, st1, move, pile, np.min(qtable[st2[0], st2[1], st2[2]]))\n",
    "    \n",
    "            else:\n",
    "                    # not immediate loss - reward it\n",
    "                qtable_update(Reward, st1, move, pile, np.max(qtable[st2[0], st2[1], st2[2]]))\n",
    "   \n",
    "            # Switch sides for play and learning\n",
    "            \n",
    "            st1 = st2\n",
    " # Equation 3 - update the qtable\n",
    "def qtable_update(r, _st1, move, pile, q_future_best):\n",
    "    a = pile*ITEMS_MX+move-1\n",
    "    qtable[_st1[0], _st1[1], _st1[2], a] = Alpha * (r + Gamma * q_future_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nim_qlearn(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games,   Random  501    Random  499\n",
      "1000 games,     Guru 1000    Random    0\n",
      "1000 games,   Random    5      Guru  995\n",
      "1000 games,     Guru  949      Guru   51\n"
     ]
    }
   ],
   "source": [
    "# Play games\n",
    "play_games(1000, 'Random', 'Random')\n",
    "play_games(1000, 'Guru', 'Random')\n",
    "play_games(1000, 'Random', 'Guru')\n",
    "play_games(1000, 'Guru', 'Guru') ;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games, Qlearner  922    Random   78\n",
      "1000 games,   Random   80  Qlearner  920\n",
      "1000 games,   Random  512    Random  488\n"
     ]
    }
   ],
   "source": [
    "# Play games\n",
    "play_games(1000, 'Qlearner', 'Random')\n",
    "play_games(1000, 'Random', 'Qlearner')\n",
    "\n",
    "play_games(1000, 'Random', 'Random') ;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 games, Qlearner  504    Random  496\n",
      "1000 games, Qlearner  585    Random  415\n",
      "1000 games, Qlearner  764    Random  236\n",
      "1000 games, Qlearner  892    Random  108\n",
      "1000 games, Qlearner  987    Random   13\n",
      "1000 games, Qlearner  999    Random    1\n",
      "1000 games, Qlearner  997    Random    3\n"
     ]
    }
   ],
   "source": [
    "# See the training size effect\n",
    "n_train = (3, 10, 100, 1000, 10000, 50000, 100000)\n",
    "wins = []\n",
    "for n in n_train:\n",
    "    nim_qlearn(n)\n",
    "    a, b = play_games(1000, 'Qlearner', 'Random')\n",
    "    wins += [a/(a+b)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.504, 0.585, 0.764, 0.892, 0.987, 0.999, 0.997]\n"
     ]
    }
   ],
   "source": [
    "# Check the ratio of wins wrt to size of the reinforcement model training\n",
    "print(wins)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print the entire set of states\n",
    "def qtable_log(_fn):\n",
    "    with open(_fn, 'w') as fout:\n",
    "        s = 'state'\n",
    "        for a in range(ITEMS_MX*3):\n",
    "            move, pile = a%ITEMS_MX+1, a//ITEMS_MX\n",
    "            s += ',%02d_%01d' % (move,pile)\n",
    "        print(s, file=fout)\n",
    "        for i, j, k in [(i,j,k) for i in range(ITEMS_MX+1) for j in range(ITEMS_MX+1) for k in range(ITEMS_MX+1)]:\n",
    "            s = '%02d_%02d_%02d' % (i,j,k)\n",
    "            for a in range(ITEMS_MX*3):\n",
    "                r = qtable[i, j, k, a]\n",
    "                s += ',%.1f' % r\n",
    "            print(s, file=fout)\n",
    "\n",
    "qtable_log('qtable_debug.txt')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
